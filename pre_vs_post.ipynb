{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipynb.fs.full.common import *\n",
    "%run common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ['Hemlata', 'Malti', 'Preeti', 'Sharifa', 'Vinita', 'VKS'] ['Geeta', 'Jitendra', 'Jyoti', 'Kuldeep', 'Seema', 'VijayLaxmi']\n",
    "# ACTIVE_SHAM = 'Active'\n",
    "# SAMPLE = 'Hemlata'\n",
    "# ELECTRODES = '32electrodes' # '32electrodes' \n",
    "GROUP1 = 'Pre'\n",
    "GROUP2 = 'Post' # generally GROUP1+1\n",
    "mapping = {0: 'theta', 1: 'alpha', 2: 'beta'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import mne\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from scipy.stats import ttest_rel\n",
    "# from mne_connectivity import spectral_connectivity_epochs\n",
    "# import networkx as nx\n",
    "# import numpy as np \n",
    "# import glob\n",
    "\n",
    "# matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2Pre-TDCS : .easy, # 3TDCS : .easy, # 4Post-TDCS : .easy\n",
    "# import glob\n",
    "# folder_path = os.path.join(os.getcwd(), 'Depression-Sample-dataset-AIIMS', ELECTRODES, ACTIVE_SHAM)\n",
    "# # folders = ['1Pre-BML', '2Pre-TDCS', '3TDCS', '4Post-TDCS', '5Post-BML']\n",
    "# # folders = ['2Pre-TDCS', '3TDCS', '4Post-TDCS']\n",
    "# all_files = []\n",
    "# # for folder in folders:\n",
    "# #     ext = '.eeg' if folder[0] in ['1', '5'] else 'Close.easy'\n",
    "# #     is_even = 1\n",
    "# #     for _ in glob.glob(os.path.join(folder_path, folder, SAMPLE) + '/*' + ext):\n",
    "# #         if (folder[0] == '3' and is_even%2==1) or folder[0] in ['2', '4']: #TODO: add for both 1 and 5 also \n",
    "# #             all_files.append(_)\n",
    "# #         is_even += 1\n",
    "\n",
    "# #     sorted_files = sorted(all_files, key=lambda x: os.path.getmtime(os.path.join(folder_path, folder, SAMPLE, x)))\n",
    "# # print(len(sorted_files), sorted_files)\n",
    "# folders = ['2Pre-TDCS', '4Post-TDCS']\n",
    "# for folder in folders:\n",
    "#     ext = 'Close.easy'\n",
    "#     for _ in glob.glob(os.path.join(folder_path, folder, SAMPLE) + '/*' + ext):\n",
    "#         all_files.append(_)\n",
    "\n",
    "# print(len(all_files), all_files)\n",
    "\n",
    "# # TODO: ADD iterator that changes Groups value and iterates (len(sorted_files) - 1 times)\n",
    "# raw_1 = data_transformation_easy(all_files[0])\n",
    "# raw_1 = g1_preprocess(raw_1)\n",
    "# raw_2 = data_transformation_easy(all_files[1])\n",
    "# raw_2 = g1_preprocess(raw_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(), 'Depression-Sample-dataset-AIIMS', ELECTRODES, ACTIVE_SHAM)\n",
    "all_files = []\n",
    "\n",
    "folders = ['2Pre-TDCS', '4Post-TDCS']\n",
    "for folder in folders:\n",
    "    ext = 'Close.easy' if ELECTRODES == '32electrodes' else 'EEG.easy'\n",
    "    for _ in glob.glob(os.path.join(folder_path, folder, SAMPLE) + '/*' + ext):\n",
    "        all_files.append(_)\n",
    "\n",
    "# TODO: ADD iterator that changes Groups value and iterates (len(sorted_files) - 1 times)\n",
    "raw_1 = data_transformation_easy(all_files[0])\n",
    "# SHOW=True\n",
    "# fig = raw_1.plot(\n",
    "#         n_channels=32, \n",
    "#         scalings=SCALINGS,\n",
    "#         show=SHOW\n",
    "#         )\n",
    "# info = raw_1.info\n",
    "\n",
    "# from scipy.io import loadmat\n",
    "# matfiles = glob.glob(os.path.join(os.getcwd(), 'Depression-Sample-dataset-AIIMS', 'Preeti_PRE_clean.mat'))\n",
    "# data = {}\n",
    "# print(matfiles)\n",
    "# dat = loadmat(matfiles[0])\n",
    "# raw_2 = np.array(dat['EEG']['data'][0][0])\n",
    "# raw_2 = mne.io.RawArray(raw_2, info)\n",
    "# fig = raw_2.plot(\n",
    "#         n_channels=32, \n",
    "#         scalings=2e2,\n",
    "#         show=SHOW\n",
    "#         )\n",
    "\n",
    "raw_1 = g1_preprocess(raw_1)\n",
    "raw_2 = data_transformation_easy(all_files[1])\n",
    "raw_2 = g1_preprocess(raw_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_file = f\"comparison-results/result-2-{ACTIVE_SHAM}.xlsx\"\n",
    "df = pd.read_excel(existing_file, header=[0,1])\n",
    "\n",
    "# main_column_names = ['index', 'swn', 'links', 'asymmetry', 'hubs', 'channels', 'regions', 'cc', 'lavg', 'eglo', 'eloc']\n",
    "# subcols = ('theta-g1', 'theta-g2', 'alpha-g1', 'alpha-g2', 'beta-g1', 'beta-g2')\n",
    "\n",
    "# subcolumn_names = {\n",
    "#     'index' : ['index'],\n",
    "#     'swn': subcols,\n",
    "#     'links': subcols,\n",
    "#     'asymmetry': subcols,\n",
    "#     'hubs': ('theta', 'alpha', 'beta'),\n",
    "#     'channels': ('theta', 'alpha', 'beta'),\n",
    "#     'regions': ('theta', 'alpha', 'beta'),\n",
    "#     'cc' : subcols,\n",
    "#     'lavg': subcols,\n",
    "#     'eglo': subcols,\n",
    "#     'eloc': subcols\n",
    "# }\n",
    "\n",
    "# columns_tuples = [(main_col, sub_col) for main_col in main_column_names for sub_col in subcolumn_names[main_col]]\n",
    "# columns = pd.MultiIndex.from_tuples(columns_tuples)\n",
    "# df = pd.DataFrame(columns=columns)\n",
    "\n",
    "row_data = {('index', 'index'): f'{ACTIVE_SHAM}-{SAMPLE}-{GROUP1}vs{GROUP2}'}\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PSD graph plot between MDD and Control group\n",
    "\n",
    "For resting state EEG channel, analyzing the overall average power across channels provides a holistic view of the brain's activity without emphasizing the specificity of individual channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 PSD using plot_psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#plot_psd uses welch method for continuous data\n",
    "raw_1.plot_psd(picks=raw_1.info['ch_names'], average=True, fmin=FMIN, fmax=FMAX, ax=ax, show=False, color='blue', dB=False)\n",
    "raw_2.plot_psd(picks=raw_2.info['ch_names'], average=True, fmin=FMIN, fmax=FMAX, ax=ax, show=False, color='red', dB=False)\n",
    "\n",
    "# dB = True plots PSD in decibels (logarithmic)\n",
    "# different n_fft compared to psd_array_welch\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Power/Frequency (microV^2/Hz)')\n",
    "ax.set_title('Power Spectral Density Comparison')\n",
    "\n",
    "ax.text(0.8, 0.9, GROUP1, color='blue', transform=ax.transAxes)\n",
    "ax.text(0.8, 0.85, GROUP2, color='red', transform=ax.transAxes)\n",
    "\n",
    "plt.ylim(0, 8)\n",
    "# plt.show()\n",
    "\n",
    "fig.savefig(f'comparison-results/PSD - {SAMPLE} - {GROUP1} vs {GROUP2}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Epoching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 4.0\n",
    "overlap = 2.0 \n",
    "\n",
    "samples_per_epoch = int(duration * SFREQ)\n",
    "samples_per_overlap = int(overlap * SFREQ)\n",
    "\n",
    "# Manually created events\n",
    "start, stop = 0, samples_per_epoch\n",
    "events = []\n",
    "while stop <= len(raw_1):\n",
    "    events.append([start, 0, 1]) \n",
    "    start += samples_per_overlap\n",
    "    stop += samples_per_overlap\n",
    "\n",
    "events = np.array(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Segmenting epochs for Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_1 = mne.Epochs(raw_1, events, tmin=0, tmax=duration, baseline=None, detrend=1,\n",
    "                    picks=None, preload=True, reject=None)\n",
    "# raw_1.plot(n_channels=len(raw_1.info['ch_names']), events=events, event_color={1:'r'}, scalings=SCALINGS)\n",
    "# epochs_1.plot(n_channels=len(raw_1.info['ch_names']), event_color={1:'r'}, events=events, scalings=SCALINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_1.plot()\n",
    "# ica = ICA(n_components=20, method='fastica', random_state=23).fit(raw_1)\n",
    "# # ica.exclude = [1]\n",
    "# # raw_clean = ica.apply(raw.copy())\n",
    "# ica.plot_components()\n",
    "# ica.plot_properties(raw_1, picks=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_2 = mne.Epochs(raw_2, events, tmin=0, tmax=duration, baseline=None, detrend=1,\n",
    "                    picks=None, preload=True, reject=None)\n",
    "# raw_2.plot(n_channels=len(raw_1.info['ch_names']), events=events, event_color={1:'r'}, scalings=SCALINGS)\n",
    "# epochs_2.plot(n_channels=len(raw_1.info['ch_names']), event_color={1:'r'}, events=events, scalings=SCALINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autoreject\n",
    "\n",
    "# ar = autoreject.AutoReject(n_interpolate=[1, 2, 3, 4], random_state=11,n_jobs=1, verbose=True)\n",
    "# ar.fit(epochs_1[:20])  # fit on a few epochs to save time\n",
    "# epochs_1, reject_log = ar.transform(epochs_1, return_log=True)\n",
    "\n",
    "\n",
    "# ar = autoreject.AutoReject(n_interpolate=[1, 2, 3, 4], random_state=11,n_jobs=1, verbose=True)\n",
    "# ar.fit(epochs_2[:20])  # fit on a few epochs to save time\n",
    "# epochs_2, reject_log = ar.transform(epochs_2, return_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amplitude plots between both groups\n",
    "epochs_1_avg = epochs_1['1'].average()\n",
    "epochs_2_avg = epochs_2['1'].average()\n",
    "# evokeds = dict(epochs_1=epochs_1)\n",
    "# fig_MDD = epochs_1_avg.plot(titles=GROUP1, time_unit = 'ms')\n",
    "# fig_control = epochs_2_avg.plot(titles=GROUP2, time_unit = 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PLI and construction of brain function matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pli method always gives positive correlations\n",
    "connectivity = {}\n",
    "def connectivity_matrix(epochs, i):\n",
    "    return spectral_connectivity_epochs(\n",
    "    epochs, method='pli', mode='multitaper', sfreq=SFREQ,\n",
    "    fmin=FREQ_BANDS[mapping[i]][0], fmax=FREQ_BANDS[mapping[i]][1], faverage=True, n_jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = len(raw_1.info['ch_names'])\n",
    "for i in mapping:\n",
    "    con = connectivity_matrix(epochs_1, i)\n",
    "    connectivity[f'{GROUP1}-{mapping[i]}'] = con.get_data().reshape((n_channels, n_channels))\n",
    "\n",
    "    con = connectivity_matrix(epochs_2, i)\n",
    "    connectivity[f'{GROUP2}-{mapping[i]}'] = con.get_data().reshape((n_channels, n_channels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Thresholding - M1 -  Small World Index\n",
    "\n",
    "Preserves small-world properties in both groups to ensure that any observed differences in connectivity are not biased by the thresholding method.\n",
    "Preserving these properties ensures that information can be transmitted quickly and effectively across the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_random_graph(n, e):\n",
    "#     adj_matrix = np.zeros((n, n))\n",
    "#     edges = [(i, j) for i in range(n) for j in range(i) if i != j]\n",
    "#     selected_edges = np.random.choice(len(edges), e, replace=False)\n",
    "#     for edge_idx in selected_edges:\n",
    "#         i, j = edges[edge_idx]\n",
    "#         adj_matrix[i, j] = 1\n",
    "#     return adj_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_symmetric(arr, n_channels):\n",
    "    for row in range(n_channels):\n",
    "        for col in range(n_channels):\n",
    "            arr[row][col] = arr[col][row]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_swn(mat, threshold):\n",
    "    tthresh = np.median(mat[mat!=0]) + threshold*np.std(mat[mat!=0])\n",
    "    print(\"*****111111******\", tthresh)\n",
    "    binmat= mat > tthresh\n",
    "    n_connections = np.count_nonzero(binmat == 1)\n",
    "    print(\"*****222222******\", n_connections)\n",
    "    binmat = make_symmetric(binmat.astype(int), n_channels)\n",
    "    g= calculate_avergae_components(nx.from_numpy_array(binmat))\n",
    "    Lw_binarized, CC_binarized = g[0], g[3]\n",
    "    return (CC_binarized/Lw_binarized if n_connections else 0), n_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOW_SIZE = 5\n",
    "# def moving_average(data, window_size = WINDOW_SIZE):\n",
    "#     cumsum = np.cumsum(data, dtype=float)\n",
    "#     cumsum[window_size:] = cumsum[window_size:] - cumsum[:-window_size]\n",
    "#     return cumsum[window_size - 1:] / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "n_perms = 1000\n",
    "n_network = n_perms//10\n",
    "# thresholds = np.linspace(0, 1.5, 200)\n",
    "swn_z_1, swn_z_2 = {}, {}\n",
    "# optimal_threshold = {}\n",
    "optimal_swn_perms, optimal_swn_real1, optimal_swn_real2 = {}, {}, {}\n",
    "# moving_avg1, moving_avg2 = {}, {}\n",
    "\n",
    "for i in mapping:\n",
    "    conmat1, conmat2 = connectivity[f'{GROUP1}-{mapping[i]}'], connectivity[f'{GROUP2}-{mapping[i]}']\n",
    "    # swn_z_1[mapping[i]], swn_z_2[mapping[i]] = 0, 0\n",
    "    # optimal_swn_perms[mapping[i]], optimal_swn_real1[mapping[i]], optimal_swn_real2[mapping[i]] = [[] for _ in range(len(thresholds))], np.zeros(len(thresholds)),  np.zeros(len(thresholds))\n",
    "    # max_diff = 0\n",
    "    if ACTIVE_SHAM == 'Active':\n",
    "        x = {'theta': 0.9422110552763826, 'alpha': 0.4824120603015075, 'beta': 1.2738693467336675}\n",
    "    else:\n",
    "        x = {'theta': 0.2788944723618091, 'alpha': 0.391959798994975, 'beta': 0.2638190954773868}\n",
    "    threshold = x[mapping[i]]\n",
    "    # for threshi, threshold in enumerate(thresholds):\n",
    "    #     print(\"00000000\", threshi, threshold)\n",
    "    swn_1, n_connections1 = calculate_swn(conmat1, threshold)\n",
    "    swn_2, n_connections2 = calculate_swn(conmat2, threshold)\n",
    "\n",
    "    # Create #n_network random matrices with vertices: n_channels and edges: average of edges of thresholded binary matrix of both groups \n",
    "    # And make a list of #n_network lists of clustering coeffs and shortest avg path length\n",
    "    # Crand, Lrand = [], []\n",
    "    # swn_perms = np.zeros(n_perms)\n",
    "    # for perm in range(n_network):\n",
    "    #     random_binmat = create_random_graph(n_channels, (n_connections1+n_connections2)//2)\n",
    "    #     random_binmat = make_symmetric(random_binmat, n_channels)\n",
    "    #     grand = calculate_avergae_components(nx.from_numpy_array(random_binmat))\n",
    "    #     Crand.append(grand[3])\n",
    "    #     Lrand.append(grand[0])\n",
    "\n",
    "    # print(\"CHECK\", Crand.count(0))\n",
    "    # print(\"CHECK2\", Lrand.count(0))\n",
    "    # # choose any 2 random network from above n_networks #n_perms times \n",
    "    # for perm in range(n_perms):\n",
    "    #     whichnetworks2use = random.sample(range(n_network), 2)\n",
    "    #     if Crand[whichnetworks2use[1]] and Lrand[whichnetworks2use[0]]:\n",
    "    #         swn_perms[perm] = (Crand[whichnetworks2use[0]] / Crand[whichnetworks2use[1]]) / (Lrand[whichnetworks2use[0]] / Lrand[whichnetworks2use[1]])\n",
    "    #     else:\n",
    "    #         perm -= 1\n",
    "    \n",
    "    # swn_rand_avg = np.mean(Crand, axis=0)/np.mean(Lrand, axis=0)\n",
    "    # swn_real_1 = swn_1 / swn_rand_avg # TODO: different random avg or same ?\n",
    "    # swn_real_2 = swn_2 / swn_rand_avg\n",
    "    # val1, val2 = (swn_real_1 - np.mean(swn_perms)) / np.std(swn_perms), (swn_real_2 - np.mean(swn_perms)) / np.std(swn_perms)\n",
    "    # swn_z_1[mapping[i]] = val1\n",
    "    # swn_z_2[mapping[i]] = val2\n",
    "\n",
    "    # max_diff = 0\n",
    "    # moving_avg1[mapping[i]] = moving_average(swn_z_1[mapping[i]])\n",
    "    # moving_avg2[mapping[i]] = moving_average(swn_z_2[mapping[i]])\n",
    "    # moving_threshold = moving_average(thresholds)\n",
    "\n",
    "    # print(\"$$$$$\", optimal_swn_perms[mapping[i]])\n",
    "    # optimal_swn_perms[mapping[i]] = swn_perms\n",
    "    row_data[('swn', f'{mapping[i]}-g1')] = swn_1 \n",
    "    row_data[('swn', f'{mapping[i]}-g2')] = swn_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # High standardized network metric suggests that our network exhibits properties away from random networks \n",
    "# for i in mapping:\n",
    "#     plt.plot(thresholds, swn_z_1[mapping[i]], '-o', markerfacecolor='w', label=f'{GROUP1}')\n",
    "#     plt.plot(thresholds, swn_z_2[mapping[i]], '-o', markerfacecolor='w', label=f'{GROUP2}')\n",
    "#     plt.xlabel(f'Threshold (Standard deviations above median) at frequency band {mapping[i]}')\n",
    "#     plt.ylabel(f'SWN_z')\n",
    "#     plt.xlim([0, 1.5])\n",
    "#     plt.legend()\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Binarization of functional connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To maximize the statistical significance of small world networkness for both groups while \n",
    "# # maximizing the difference between the z-normalized small world networks of the two groups.\n",
    "# f, axarr = plt.subplots(1,3, figsize=(100, 5)) \n",
    "# for count, i in enumerate(mapping):\n",
    "#     max_diff = 0\n",
    "#     for mov in range(len(moving_threshold)):\n",
    "#         x = np.abs(moving_avg1[mapping[i]][mov] - moving_avg2[mapping[i]][mov])\n",
    "#         if x>max_diff:\n",
    "#             max_diff = x\n",
    "#             optimal_threshold[mapping[i]] = moving_threshold[mov]\n",
    "#             index = mov\n",
    "\n",
    "#     # TODO: save fig file\n",
    "#     p_value_1 = np.mean(optimal_swn_perms[mapping[i]][index+(WINDOW_SIZE//2)] >= optimal_swn_real1[mapping[i]][index+(WINDOW_SIZE//2)])\n",
    "#     p_value_2 = np.mean(optimal_swn_perms[mapping[i]][index+(WINDOW_SIZE//2)] >= optimal_swn_real2[mapping[i]][index+(WINDOW_SIZE//2)])\n",
    "#     print(mapping[i], \"p-value: \", p_value_1, p_value_2)\n",
    "\n",
    "\n",
    "#     axarr[count].hist(optimal_swn_perms[mapping[i]][index+(WINDOW_SIZE//2)], bins=50, color='b', alpha=0.7)\n",
    "#     axarr[count].axvline(optimal_swn_real1[mapping[i]][mov+(WINDOW_SIZE//2)], color='m', linewidth=3, label=f'{GROUP1}')\n",
    "#     axarr[count].axvline(optimal_swn_real2[mapping[i]][mov+(WINDOW_SIZE//2)], color='r', linewidth=3, label=f'{GROUP2}')\n",
    "#     axarr[count].set_xlabel(f'{mapping[i]}')\n",
    "#     axarr[count].set_xlim([0,10])\n",
    "#     # axarr[count].set_ylim([0,100])\n",
    "#     axarr[count].legend()\n",
    "# f.suptitle('Random Network distribution for both Pre and Post groups')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threhsold\n",
    "binarized_matrix = {}\n",
    "print(x[mapping[i]])\n",
    "for i in mapping:\n",
    "    conmat1, conmat2 = connectivity[f'{GROUP1}-{mapping[i]}'], connectivity[f'{GROUP2}-{mapping[i]}']\n",
    "    tthresh1 = np.median(conmat1[conmat1!=0]) + x[mapping[i]]*np.std(conmat1[conmat1!=0])\n",
    "    tthresh2 = np.median(conmat2[conmat2!=0]) + x[mapping[i]]*np.std(conmat2[conmat2!=0])\n",
    "    binarized_matrix[f'{GROUP1}-{mapping[i]}'] = connectivity[f'{GROUP1}-{mapping[i]}'] > tthresh1\n",
    "    binarized_matrix[f'{GROUP2}-{mapping[i]}'] = connectivity[f'{GROUP2}-{mapping[i]}'] > tthresh2\n",
    "    print(tthresh1, tthresh2)\n",
    "print(binarized_matrix.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find number of links in each graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mapping:\n",
    "    links_1 = 0\n",
    "    links_2 = 0\n",
    "\n",
    "    for row in range(n_channels):\n",
    "        for col in range(row+1):\n",
    "            if binarized_matrix[f'{GROUP1}-{mapping[i]}'][row][col] == 1:\n",
    "                links_1+=1\n",
    "            if binarized_matrix[f'{GROUP2}-{mapping[i]}'][row][col] == 1:\n",
    "                links_2+=1\n",
    "\n",
    "    print(f'{GROUP1}-{mapping[i]}-number of links: K=', links_1)\n",
    "    print(f'{GROUP2}-{mapping[i]}-number of links: K=', links_2)\n",
    "\n",
    "    row_data[('links', f'{mapping[i]}-g1')] = links_1\n",
    "    row_data[('links', f'{mapping[i]}-g2')] = links_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Asymmetry in  each band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_region = ['Fp1', 'AF3', 'PO3','O1', 'CP1', 'FC1', 'P3', 'C3', 'F3', 'F7', 'FC5', 'CP5', 'T7', 'P7']\n",
    "right_region = ['P8', 'T8', 'CP6', 'FC6', 'F8', 'F4', 'C4', 'P4', 'AF4', 'Fp2', 'FC2', 'CP2', 'O2', 'PO4']\n",
    "\n",
    "for i in mapping:\n",
    "    lr = 1\n",
    "    rr = 1\n",
    "    lrc = 1\n",
    "    rrc = 1\n",
    "    for row in range(n_channels):\n",
    "        for col in range(row+1):\n",
    "            if binarized_matrix[f'{GROUP1}-{mapping[i]}'][row][col] == 1:\n",
    "                if raw_1.info['ch_names'][row] in left_region or raw_1.info['ch_names'][col] in left_region:\n",
    "                    lr += 1\n",
    "                if raw_1.info['ch_names'][row] in right_region or raw_1.info['ch_names'][col] in right_region:\n",
    "                    rr += 1\n",
    "                # print(raw_1.info['ch_names'][row], raw_1.info['ch_names'][col])\n",
    "\n",
    "            if binarized_matrix[f'{GROUP2}-{mapping[i]}'][row][col] == 1:\n",
    "                if raw_1.info['ch_names'][row] in left_region or raw_1.info['ch_names'][col] in left_region:\n",
    "                    lrc += 1\n",
    "                if raw_1.info['ch_names'][row] in right_region or raw_1.info['ch_names'][col] in right_region:\n",
    "                    rrc += 1\n",
    "                # print(raw_1.info['ch_names'][row], raw_1.info['ch_names'][col])\n",
    "\n",
    "    print(f\"{GROUP1}: \",mapping[i], \"left region: \", lr, \"right region: \", rr, \"proportion: \", lr/rr)\n",
    "    print(f\"{GROUP2}: \", mapping[i], \"left region: \", lrc, \"right region: \", rrc, \"proportion: \", lrc/rrc)\n",
    "\n",
    "    row_data[('asymmetry', f'{mapping[i]}-g1')] = lr/rr\n",
    "    row_data[('asymmetry', f'{mapping[i]}-g2')] = lrc/rrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,3) \n",
    "for count, i in enumerate(mapping):\n",
    "    conmat1 = make_symmetric(connectivity[f'{GROUP1}-{mapping[i]}'], n_channels)\n",
    "    # conmat2 = make_symmetric(connectivity[f'{GROUP2}-{mapping[i]}'], n_channels)\n",
    "\n",
    "    im = axarr[count].imshow(conmat1)\n",
    "    axarr[count].set_title(f'Functional connectivity Pre {mapping[i]} in {ACTIVE_SHAM} treatment')\n",
    "    axarr[count].set_xticks([_ for _ in range(n_channels)])\n",
    "    axarr[count].set_xticklabels(raw_1.info['ch_names'], rotation='vertical')\n",
    "    axarr[count].set_yticks([_ for _ in range(n_channels)])\n",
    "    axarr[count].set_yticklabels(raw_1.info['ch_names'], rotation='horizontal')\n",
    "    axarr[count].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "f.text(0.5, 0.04, 'X-axis label', ha='center')\n",
    "f.text(0.04, 0.5, 'Y-axis label', va='center', rotation='vertical')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,3) \n",
    "for count, i in enumerate(mapping):\n",
    "    # conmat1 = make_symmetric(connectivity[f'{GROUP1}-{mapping[i]}'], n_channels)\n",
    "    conmat2 = make_symmetric(connectivity[f'{GROUP2}-{mapping[i]}'], n_channels)\n",
    "\n",
    "    im = axarr[count].imshow(conmat1)\n",
    "    axarr[count].set_title(f'Functional connectivity Post {mapping[i]} in {ACTIVE_SHAM} treatment')\n",
    "    axarr[count].set_xticks([_ for _ in range(n_channels)])\n",
    "    axarr[count].set_xticklabels(raw_1.info['ch_names'], rotation='vertical')\n",
    "    axarr[count].set_yticks([_ for _ in range(n_channels)])\n",
    "    axarr[count].set_yticklabels(raw_1.info['ch_names'], rotation='horizontal')\n",
    "    axarr[count].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "f.text(0.5, 0.04, 'X-axis label', ha='center')\n",
    "f.text(0.04, 0.5, 'Y-axis label', va='center', rotation='vertical')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Difference matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_index(channels_list, raw):\n",
    "    indexes = []\n",
    "    for channel in channels_list:\n",
    "        for i in range(len(raw.info['ch_names'])):\n",
    "            if raw.info['ch_names'][i]==channel:\n",
    "                indexes.append(i)\n",
    "    return(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_channels = return_index(['Pz', 'Oz', 'Fz', 'Cz'], raw_1)\n",
    "central_channels\n",
    "raw_1.info['bads'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_matrix = {}\n",
    "for i in mapping:\n",
    "    # TODO: both difference and excel it in in different column\n",
    "    difference_matrix[mapping[i]] = binarized_matrix[f'{GROUP2}-{mapping[i]}'].astype(int) - binarized_matrix[f'{GROUP1}-{mapping[i]}'].astype(int) \n",
    "    # difference_matrix[mapping[i]] = binarized_matrix[f'{GROUP1}-{mapping[i]}'].astype(int) - binarized_matrix[f'{GROUP2}-{mapping[i]}'].astype(int)\n",
    "    for k in range(n_channels):\n",
    "        for l in range(n_channels):\n",
    "            if difference_matrix[mapping[i]][k][l] == -1 or k in central_channels or l in central_channels:\n",
    "                difference_matrix[mapping[i]][k][l] = 0\n",
    "\n",
    "    difference_matrix[mapping[i]] = make_symmetric(difference_matrix[mapping[i]], n_channels)\n",
    "\n",
    "    # # Yellow - Higher synchronisation\n",
    "    # diff_mat = difference_matrix[mapping[i]]\n",
    "    # plt.imshow(diff_mat, interpolation='none')\n",
    "    # plt.title(f'Binary FC difference in {ACTIVE_SHAM} treatment in {mapping[i]}')\n",
    "    # plt.xticks([_ for _ in range(n_channels)], raw_1.info['ch_names'], rotation='vertical')\n",
    "    # plt.yticks([_ for _ in range(n_channels)], raw_1.info['ch_names'], rotation='horizontal')\n",
    "    # plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "    # plot_sensors_connectivity(\n",
    "    #     raw_1.info,\n",
    "    #     difference_matrix[mapping[i]],\n",
    "    #     cbar_label=f'{mapping[i]}-Connectivity',\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm, colors\n",
    "f, axarr = plt.subplots(1,3) \n",
    "for count, i in enumerate(mapping):\n",
    "    conmat1 = make_symmetric((connectivity[f'{GROUP2}-{mapping[i]}'] - connectivity[f'{GROUP1}-{mapping[i]}']), n_channels)\n",
    "\n",
    "    im = axarr[count].imshow(conmat1)\n",
    "    axarr[count].set_title(f'FCdifferent maaatrix {mapping[i]} in {ACTIVE_SHAM} treatment')\n",
    "    axarr[count].set_xticks([_ for _ in range(n_channels)])\n",
    "    axarr[count].set_xticklabels(raw_1.info['ch_names'], rotation='vertical')\n",
    "    axarr[count].set_yticks([_ for _ in range(n_channels)])\n",
    "    axarr[count].set_yticklabels(raw_1.info['ch_names'], rotation='horizontal')\n",
    "    axarr[count].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "f.colorbar(cm.ScalarMappable(norm=colors.Normalize(0, 5), cmap=cm.get_cmap(\"Spectral\")), ax=ax)\n",
    "f.text(0.5, 0.04, 'X-axis label', ha='center')\n",
    "f.text(0.04, 0.5, 'Y-axis label', va='center', rotation='vertical')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hub(mat):\n",
    "        hub =[]\n",
    "        for ch, ch_name in enumerate(raw_1.info['ch_names']):\n",
    "            hub.append(np.count_nonzero(mat[ch] == 1))\n",
    "        return hub\n",
    "        # layout = find_layout(raw_1.info)\n",
    "        # plot_topomap(hub, layout.pos, names=raw_1.info['ch_names'], sphere='eeglab')\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_threshold = {}\n",
    "for i in mapping:\n",
    "    hub = calculate_hub(difference_matrix[mapping[i]])\n",
    "    edges_threshold[mapping[i]] = np.median(hub) + np.std(hub)\n",
    "    row_data[('hubs', mapping[i])] = hub\n",
    "print(edges_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_channels = {}\n",
    "for i in mapping:\n",
    "    selected_channels[mapping[i]] = []\n",
    "    for ch in range(n_channels):\n",
    "        if len(nx.from_numpy_array(difference_matrix[mapping[i]]).edges(ch)) > edges_threshold[mapping[i]]:\n",
    "            selected_channels[mapping[i]].append(raw_1.info['ch_names'][ch])\n",
    "print(selected_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary of Regions in brain vs channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionvschannel = {\n",
    "    'LC': ['C3', 'CP1', 'CP5', 'FC1', 'FC5'], # Left central \n",
    "    'LF': ['FP1', 'AF3', 'F3', 'F7'], # Left frontal\n",
    "    'LT': ['T7'], # Left temporal\n",
    "    'LPO': ['PO3',  'P3', 'P7', 'O1'], # Left parietal-occipital\n",
    "    'RC': ['CP6', 'FC6', 'C4', 'FC2', 'CP2',], # Right central \n",
    "    'RF': ['F8', 'F4', 'AF4', 'FP2'], # Right frontal\n",
    "    'RT': ['T8'], # Right temporal\n",
    "    'RPO': ['P8', 'P4', 'PO4', 'O2'] , # Right parietal-occipital\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_to_region = {}\n",
    "for region, region_channels in regionvschannel.items():\n",
    "    for channel in region_channels:\n",
    "        channel_to_region[channel] = region\n",
    "\n",
    "selected_regions = {}\n",
    "for band, channels in selected_channels.items():\n",
    "    regions = set()\n",
    "    for channel in channels:\n",
    "        if channel in channel_to_region:\n",
    "            regions.add(channel_to_region[channel])\n",
    "    selected_regions[band] = list(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mapping:\n",
    "    row_data[('channels', mapping[i])] = selected_channels[mapping[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mapping:\n",
    "    row_data[('regions', mapping[i])] = selected_regions[mapping[i]]\n",
    "row_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD EXTRAS IN EXCEL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mapping:\n",
    "    g1 = calculate_avergae_components(nx.from_numpy_array(binarized_matrix[f'{GROUP2}-{mapping[i]}']))\n",
    "    g2 = calculate_avergae_components(nx.from_numpy_array(binarized_matrix[f'{GROUP2}-{mapping[i]}']))\n",
    "\n",
    "    row_data[('cc', f'{mapping[i]}-g1')] = g1[3]\n",
    "    row_data[('cc', f'{mapping[i]}-g2')] = g2[3]\n",
    "    row_data[('lavg', f'{mapping[i]}-g1')] = g1[0]\n",
    "    row_data[('lavg', f'{mapping[i]}-g2')] = g2[0]\n",
    "    row_data[('nbc', f'{mapping[i]}-g1')] = g1[1]\n",
    "    row_data[('nbc', f'{mapping[i]}-g2')] = g2[1]\n",
    "    row_data[('eglo', f'{mapping[i]}-g1')] = g1[2]\n",
    "    row_data[('eglo', f'{mapping[i]}-g2')] = g2[2]\n",
    "    row_data[('eloc', f'{mapping[i]}-g1')] = g1[4]\n",
    "    row_data[('eloc', f'{mapping[i]}-g2')] = g2[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in mapping:\n",
    "#     g1 = calculate_avergae_components(nx.from_numpy_array(connectivity_matrix[f'{GROUP1}-{mapping[i]}']))\n",
    "#     g2 = calculate_avergae_components(nx.from_numpy_array(connectivity_matrix[f'{GROUP2}-{mapping[i]}']))\n",
    "\n",
    "#     row_data[('swn', f'{mapping[i]}-g1')] = g1[3]/g1[0]\n",
    "#     row_data[('swn', f'{mapping[i]}-g2')] = g2[3]/g2[0]\n",
    "#     row_data[('cc', f'{mapping[i]}-g1')] = g1[3]\n",
    "#     row_data[('cc', f'{mapping[i]}-g2')] = g2[3]\n",
    "#     row_data[('lavg', f'{mapping[i]}-g1')] = g1[0]\n",
    "#     row_data[('lavg', f'{mapping[i]}-g2')] = g2[0]\n",
    "#     row_data[('nbc', f'{mapping[i]}-g1')] = g1[1]\n",
    "#     row_data[('nbc', f'{mapping[i]}-g2')] = g2[1]\n",
    "#     row_data[('eglo', f'{mapping[i]}-g1')] = g1[2]\n",
    "#     row_data[('eglo', f'{mapping[i]}-g2')] = g2[2]\n",
    "#     row_data[('eloc', f'{mapping[i]}-g1')] = g1[4]\n",
    "#     row_data[('eloc', f'{mapping[i]}-g2')] = g2[4]\n",
    "#     row_data[('hub', f'{mapping[i]}-g1')] = g1[5]\n",
    "#     row_data[('hub', f'{mapping[i]}-g2')] = g2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row_data, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(df.index)\n",
    "new_row = pd.Series(row_data)\n",
    "print(\"length\", len(df))\n",
    "print(\"new row\", new_row)\n",
    "df.loc[len(df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset the column index\n",
    "df.to_excel(existing_file, index=True, header=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
